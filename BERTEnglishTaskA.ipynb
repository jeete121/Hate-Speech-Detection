{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERTEnglishTaskA.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeF_gt3eup5q",
        "colab_type": "code",
        "outputId": "4a9be107-ee44-42fa-8ce4-221350ba8251",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPg-Fhc6Kbxj",
        "colab_type": "code",
        "outputId": "b442d3dc-3033-4524-a0c4-131eec2d8e43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import torch\n",
        "if torch.cuda.is_available():    \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla K80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvlH2xtxKl0Q",
        "colab_type": "code",
        "outputId": "32077c01-7111-4e22-e725-2cdcd06eca69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.5.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.15)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.15)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (2.6.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZm6DIbsKrVg",
        "colab_type": "code",
        "outputId": "dc6585f6-2003-4161-ee73-313277c2d3fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 703
        }
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/drive/My Drive/MinorProject/eng_train.csv\")\n",
        "df=df.drop(columns=['ID','Sub-task B'])\n",
        "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
        "df.sample(20)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training sentences: 4,263\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Sub-task A</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3883</th>\n",
              "      <td>great show</td>\n",
              "      <td>NAG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2439</th>\n",
              "      <td>There just problem with the movie and where I ...</td>\n",
              "      <td>NAG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1786</th>\n",
              "      <td>Rohit barman nice</td>\n",
              "      <td>NAG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2329</th>\n",
              "      <td>Totally agreed with your practicality but ther...</td>\n",
              "      <td>NAG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2567</th>\n",
              "      <td>From today onward she will be called kungfu bitc?</td>\n",
              "      <td>NAG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3534</th>\n",
              "      <td>very good</td>\n",
              "      <td>NAG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1572</th>\n",
              "      <td>&lt;https://youtu.be/ghDIOlPnaA4&gt;</td>\n",
              "      <td>NAG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1393</th>\n",
              "      <td>Very true ! And best analysis covering all thi...</td>\n",
              "      <td>NAG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1830</th>\n",
              "      <td>I want to ask Singer like Sona Mohapatra But t...</td>\n",
              "      <td>CAG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2339</th>\n",
              "      <td>Koyel Di rite and very nice</td>\n",
              "      <td>NAG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1880</th>\n",
              "      <td>Amazing</td>\n",
              "      <td>NAG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3329</th>\n",
              "      <td>U deserve more subscribers sir</td>\n",
              "      <td>NAG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2144</th>\n",
              "      <td>Fake Feminists r the followers of napoleon's w...</td>\n",
              "      <td>OAG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1618</th>\n",
              "      <td>@Jagdish Jena well you did</td>\n",
              "      <td>NAG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>650</th>\n",
              "      <td>This is soo true 💯💯💯💯💯❤ bhai..u are best..</td>\n",
              "      <td>NAG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2852</th>\n",
              "      <td>Should be cut the penies those criminals</td>\n",
              "      <td>CAG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1261</th>\n",
              "      <td>if she was a beggar how can she know English,,...</td>\n",
              "      <td>NAG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3833</th>\n",
              "      <td>Kabir singh emotional Love movie👍👍👍👍👍👌👌👌👌👌</td>\n",
              "      <td>NAG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3016</th>\n",
              "      <td>Great going👍👍</td>\n",
              "      <td>NAG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3621</th>\n",
              "      <td>Excellent</td>\n",
              "      <td>NAG</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   Text Sub-task A\n",
              "3883                                         great show        NAG\n",
              "2439  There just problem with the movie and where I ...        NAG\n",
              "1786                                  Rohit barman nice        NAG\n",
              "2329  Totally agreed with your practicality but ther...        NAG\n",
              "2567  From today onward she will be called kungfu bitc?        NAG\n",
              "3534                                          very good        NAG\n",
              "1572                     <https://youtu.be/ghDIOlPnaA4>        NAG\n",
              "1393  Very true ! And best analysis covering all thi...        NAG\n",
              "1830  I want to ask Singer like Sona Mohapatra But t...        CAG\n",
              "2339                        Koyel Di rite and very nice        NAG\n",
              "1880                                            Amazing        NAG\n",
              "3329                     U deserve more subscribers sir        NAG\n",
              "2144  Fake Feminists r the followers of napoleon's w...        OAG\n",
              "1618                         @Jagdish Jena well you did        NAG\n",
              "650          This is soo true 💯💯💯💯💯❤ bhai..u are best..        NAG\n",
              "2852           Should be cut the penies those criminals        CAG\n",
              "1261  if she was a beggar how can she know English,,...        NAG\n",
              "3833         Kabir singh emotional Love movie👍👍👍👍👍👌👌👌👌👌        NAG\n",
              "3016                                      Great going👍👍        NAG\n",
              "3621                                          Excellent        NAG"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPgTp6uTMSYm",
        "colab_type": "code",
        "outputId": "24364b46-4760-4b1b-bc58-24e9feb1d928",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df['Sub-task A'] = df['Sub-task A'].map({'CAG': 0, 'NAG':1,'OAG':2})\n",
        "df.head()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Sub-task A</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Next part</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Iii8mllllllm\\nMdxfvb8o90lplppi0005</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>🤣🤣😂😂🤣🤣🤣😂osm vedio ....keep it up...make more v...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What the fuck was this? I respect shwetabh and...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Concerned authorities should bring arundathi R...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Text  Sub-task A\n",
              "0                                          Next part           1\n",
              "1                 Iii8mllllllm\\nMdxfvb8o90lplppi0005           1\n",
              "2  🤣🤣😂😂🤣🤣🤣😂osm vedio ....keep it up...make more v...           1\n",
              "3  What the fuck was this? I respect shwetabh and...           1\n",
              "4  Concerned authorities should bring arundathi R...           1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-cQ1WmRMqSK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df=df.rename(columns={\"Sub-task A\":\"label\"})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsm2ejSgLCPg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = df.Text.values\n",
        "labels = df.label.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56qGkZePL5r1",
        "colab_type": "code",
        "outputId": "3d56d88d-14ad-4e37-e7c5-21e26558aeff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "sentences"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Next part', 'Iii8mllllllm\\nMdxfvb8o90lplppi0005',\n",
              "       '🤣🤣😂😂🤣🤣🤣😂osm vedio ....keep it up...make more vedios like this',\n",
              "       ...,\n",
              "       'People may criticize Pratik Borade by saying that he takes movies too\\nseriously and always talks about ideologies instead of focusing on technical\\naspects. But ideology IS the most important thing. People in India are\\ninfluenced by movies more than anything else. So if a movie is trying to\\npromote negativity in society subliminally or openly it needs to be called out\\nand criticised openly. Some will say \"Oh it is just a film.\" But many people\\nDO take these movies seriously and it does affect as well as change their\\nlives. So though I disagree with Pratik regarding some movies, I hugely\\nrespect what he does. Filmmakers should not be allowed to create whatever\\nbullshit they want to manipulate minds of youths, simply in the name of\\nartistic freedom!',\n",
              "       '@Naaz Sk hello',\n",
              "       'We want to read your book sir, please make it available.'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQzqBtunMEXT",
        "colab_type": "code",
        "outputId": "4400875d-55b2-449e-9d18-9eba0e828c85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "labels"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, ..., 0, 1, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7SnHXbKNG-4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0cQ186VNYLn",
        "colab_type": "code",
        "outputId": "e8314c81-27ca-4255-d3d7-8bf80e515d60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "input_ids = []\n",
        "for sent in sentences:\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent, \n",
        "                        add_special_tokens = True,\n",
        "                   )\n",
        "    input_ids.append(encoded_sent)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (676 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (997 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (812 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (606 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQm55nf7NnI3",
        "colab_type": "code",
        "outputId": "5fb60624-2334-4c9a-afe2-fd813eecc3b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('Max sentence length: ', max([len(sen) for sen in input_ids]))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max sentence length:  999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUWPE3qXN7jo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "MAX_LEN = 100\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                          value=0, truncating=\"post\", padding=\"post\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2plWstlOCzb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "attention_masks = []\n",
        "for sent in input_ids:\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "    attention_masks.append(att_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PjHSmAYOHB5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
        "                                                            random_state=42, test_size=0.1)\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\n",
        "                                             random_state=42, test_size=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXIeTWjvOQ-p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijxbNY5wOUT6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "batch_size = 32\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tU41xhhOY96",
        "colab_type": "code",
        "outputId": "372b7878-8279-4cea-f7b0-b2ff9824d089",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    num_labels = 3, \n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False,\n",
        ")\n",
        "model.cuda()"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oHbBOoyOcnq",
        "colab_type": "code",
        "outputId": "d7527086-7660-4e60-dab9-d57b19552010",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        }
      },
      "source": [
        "params = list(model.named_parameters())\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "print('==== Embedding Layer ====\\n')\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The BERT model has 201 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
            "bert.embeddings.position_embeddings.weight                (512, 768)\n",
            "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
            "bert.embeddings.LayerNorm.weight                              (768,)\n",
            "bert.embeddings.LayerNorm.bias                                (768,)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
            "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
            "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
            "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
            "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
            "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
            "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
            "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
            "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
            "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "bert.pooler.dense.weight                                  (768, 768)\n",
            "bert.pooler.dense.bias                                        (768,)\n",
            "classifier.weight                                           (3, 768)\n",
            "classifier.bias                                                 (3,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wg7AYs7ZOolL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5,\n",
        "                  eps = 1e-8\n",
        "                )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1T4L7XvOtdL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "epochs = 4\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0,\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifH3p_kUOwtb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sSwDDcaO0oq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iZ11umBO3Vq",
        "colab_type": "code",
        "outputId": "f3aaa43e-5f44-43ec-8166-1e7b4b58fe23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        }
      },
      "source": [
        "import random\n",
        "seed_val = 42\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "loss_values = []\n",
        "for epoch_i in range(0, epochs):\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "    t0 = time.time()\n",
        "    total_loss = 0\n",
        "    model.train()\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        model.zero_grad()        \n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        loss = outputs[0]\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    loss_values.append(avg_train_loss)\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "    t0 = time.time()\n",
        "    model.eval()\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "    for batch in validation_dataloader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        with torch.no_grad():\n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        logits = outputs[0]\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "        nb_eval_steps += 1\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    120.    Elapsed: 0:00:39.\n",
            "  Batch    80  of    120.    Elapsed: 0:01:19.\n",
            "\n",
            "  Average training loss: 0.63\n",
            "  Training epcoh took: 0:01:59\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.83\n",
            "  Validation took: 0:00:05\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    120.    Elapsed: 0:00:39.\n",
            "  Batch    80  of    120.    Elapsed: 0:01:19.\n",
            "\n",
            "  Average training loss: 0.43\n",
            "  Training epcoh took: 0:01:58\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.82\n",
            "  Validation took: 0:00:05\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    120.    Elapsed: 0:00:39.\n",
            "  Batch    80  of    120.    Elapsed: 0:01:18.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epcoh took: 0:01:58\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.80\n",
            "  Validation took: 0:00:05\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    120.    Elapsed: 0:00:39.\n",
            "  Batch    80  of    120.    Elapsed: 0:01:18.\n",
            "\n",
            "  Average training loss: 0.24\n",
            "  Training epcoh took: 0:01:57\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.80\n",
            "  Validation took: 0:00:05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6LcOaGXPBo7",
        "colab_type": "code",
        "outputId": "18b80626-43f3-4471-8194-eb3440caf7c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "import seaborn as sns\n",
        "sns.set(style='darkgrid')\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "plt.plot(loss_values, 'b-o')\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAvAAAAGaCAYAAABpIXfbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wC7BjpRMl7PUAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVzhAe8cS-Bv",
        "colab_type": "code",
        "outputId": "85cd4ba3-f6ae-4957-cb91-b68fe008f678",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        }
      },
      "source": [
        "df1 = pd.read_csv(\"/content/drive/My Drive/MinorProject/eng_dev.csv\")\n",
        "df1=df1.drop(columns=['ID','Sub-task B'])\n",
        "print('Number of test sentences: {:,}\\n'.format(df1.shape[0]))\n",
        "df1['Sub-task A'] = df1['Sub-task A'].map({'CAG': 0, 'NAG':1,'OAG':2})\n",
        "df1"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of test sentences: 1,066\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Sub-task A</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>U deserve more subscribers. U really great.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Nice video....</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>sorry if i bother somebody.. iam a defence asp...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Joker was amazing....it was not glamorised !.....</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Nice baro</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1061</th>\n",
              "      <td>Ranu Mandal is third class [woman.No](http://w...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1062</th>\n",
              "      <td>[15:23](https://www.youtube.com/watch?v=N_ZMfQ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1063</th>\n",
              "      <td>Love u bro keep it up</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1064</th>\n",
              "      <td>May I help you?</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1065</th>\n",
              "      <td>You are great sir !!! When a i saw a movie fir...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1066 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   Text  Sub-task A\n",
              "0           U deserve more subscribers. U really great.           1\n",
              "1                                        Nice video....           1\n",
              "2     sorry if i bother somebody.. iam a defence asp...           1\n",
              "3     Joker was amazing....it was not glamorised !.....           1\n",
              "4                                             Nice baro           1\n",
              "...                                                 ...         ...\n",
              "1061  Ranu Mandal is third class [woman.No](http://w...           2\n",
              "1062  [15:23](https://www.youtube.com/watch?v=N_ZMfQ...           1\n",
              "1063                              Love u bro keep it up           1\n",
              "1064                                    May I help you?           1\n",
              "1065  You are great sir !!! When a i saw a movie fir...           1\n",
              "\n",
              "[1066 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTAScmrfTFkB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df1=df1.rename(columns={\"Sub-task A\":\"label\"})\n",
        "sentences = df1.Text.values\n",
        "labels = df1.label.values\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0IbWyQASypi",
        "colab_type": "code",
        "outputId": "144390bf-32fb-47ff-f513-eea2fdf691bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "input_ids = []\n",
        "for sent in sentences:\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                     \n",
        "                        add_special_tokens = True, \n",
        "                                           )\n",
        "    \n",
        "    input_ids.append(encoded_sent)\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n",
        "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "attention_masks = []\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask) \n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "prediction_labels = torch.tensor(labels)\n",
        "batch_size = 32  \n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (576 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aCboXtgTQb_",
        "colab_type": "code",
        "outputId": "ca33baf9-f8e2-4f48-b636-eecd26a81dee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
        "model.eval()\n",
        "predictions , true_labels = [], []\n",
        "for batch in prediction_dataloader:\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  b_input_ids, b_input_mask, b_labels = batch  \n",
        "  with torch.no_grad():\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  predictions.append(logits) \n",
        "  true_labels.append(label_ids)\n"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 1,066 test sentences...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mt0JAwzVTb7W",
        "colab_type": "code",
        "outputId": "1d1e632d-dc08-49ac-c729-d840d409f801",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('Positive samples: %d of %d (%.2f%%)' % (df.label.sum(), len(df.label), (df.label.sum() / len(df.label) * 100.0)))"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive samples: 4245 of 4263 (99.58%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dj6V-tpTguG",
        "colab_type": "code",
        "outputId": "235c33c4-9c2b-47d7-dd45-3d8defe18771",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "clf_set = []\n",
        "for i in range(len(true_labels)):\n",
        "  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "  clf = classification_report(true_labels[i], pred_labels_i)                \n",
        "  clf_set.append(clf)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J62sxVGeTqMn",
        "colab_type": "code",
        "outputId": "91b4517f-2f7c-431a-9572-e2ea456f9abb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "clf = classification_report(flat_true_labels, flat_predictions)\n",
        "print(clf)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.28      0.30       117\n",
            "           1       0.88      0.91      0.89       836\n",
            "           2       0.57      0.48      0.52       113\n",
            "\n",
            "    accuracy                           0.80      1066\n",
            "   macro avg       0.59      0.56      0.57      1066\n",
            "weighted avg       0.78      0.80      0.79      1066\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhYGDUhDTvC3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}